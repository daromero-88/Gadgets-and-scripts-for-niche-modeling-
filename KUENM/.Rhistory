install.packages("rJava") #source reinstalling
library (rJava) #read package
install.packages("rJava", type = 'source') #source reinstalling
library (rJava) #read package
library(ENMTools)
install.packages('rJava')
library (rJava) #read package
install.packages("rJava", type = 'source') #source reinstalling
uninstall.packages('rJava')
uninstall.packages('rJava')
install.packages('rJava')
library("rJava", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library("rjson", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
detach("package:rjson", unload=TRUE)
detach("package:rJava", unload=TRUE)
version
install.packages('rJava')
library (rJava)
install.packages('rJava')
library (rJava)
install.packages('rJava')
library (rJava)
install.packages('rJava')
install.packages('ntbox')
library(ntbox)
install.packages('ntbox')
library(raster)
library(sp)
library(rgeos)
library (rgdal)
library (maptools)
library(mapdata)
library (kuenm)
library (dismo)
library(spThin)
library (ENMeval)
destination = '/Volumes/Coral1/America_indices/7_July/EVI'
all = list.files ("/Volumes/Coral1/America_indices/MODIS",
recursive = T,
full.names = T,
pattern = '-07-.*\\_EVI.tif')
all
file.copy (all, destination)
destination = '/Volumes/Coral1/America_indices/8_August/NDVI'
all = list.files ("/Volumes/Coral1/America_indices/MODIS",
recursive = T,
full.names = T,
pattern = '-08-.*\\_NDVI.tif')
all
destination = '/Volumes/Coral1/America_indices/8_August/NDVI'
file.copy (all, destination)
destination = '/Volumes/Coral1/America_indices/8_August/EVI'
all
all = list.files ("/Volumes/Coral1/America_indices/MODIS",
recursive = T,
full.names = T,
pattern = '-08-.*\\_EVI.tif')
all
destination = '/Volumes/Coral1/America_indices/8_August/EVI'
file.copy (all, destination)
install.packages("gtrendsR")
library (gtrendsR)
destination = '/Volumes/Coral1/America_indices/9_September/NDVI'
all = list.files ("/Volumes/Coral1/America_indices/MODIS",
recursive = T,
full.names = T,
pattern = '-09-.*\\_NDVI.tif')
all
destination = '/Volumes/Coral1/America_indices/9_September/NDVI'
file.copy (all, destination)
destination = '/Volumes/Coral1/America_indices/9_September/EVI'
all = list.files ("/Volumes/Coral1/America_indices/MODIS",
recursive = T,
full.names = T,
pattern = '-09-.*\\_EVI.tif')
all
destination = '/Volumes/Coral1/America_indices/9_September/EVI'
file.copy (all, destination)
library(datetimeutils) #changing_dates
install.packages("dygraphs")
library (dygraphs)
dygraph(nhtemp, main = "New Haven Temperatures") %>%
dyRangeSelector()
nhtemp
class(nhtemp)
library (plotly)
tm <- seq(0, 600, by = 10)
x <- today - tm
y <- rnorm(length(x))
tm <- seq(0, 600, by = 10)
x <- today - tm
x <- seq (1:200)
y <- rnorm(length(x))
fig <- plot_ly(x = ~x, y = ~y, mode = 'lines', text = paste(tm, "days from today"))
fig
install.packages('htmlwidgets')
install.packages("htmlwidgets")
install.packages("htmlwidgets")
version()
version
update.packages("rmarkdown")
library (devtools)
library(raster)
library(sp)
library(rgeos)
library (rgdal)
library (maptools)
library(mapdata)
library (dismo) #SDM algorithms plus other functions
library(spThin) #thinning of occurrences
library (ENMeval) #maxnet algorithm
library (kuenm) #maxent algorithm dissected
library (virtualspecies) #create virtual species
library(corrplot) #create correlation matrices
library (gatepoints) #for advance functions
library (rgl)
library (Rcpp)
library (hypervolume) #hypervolume based models
setwd ('/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling-')
data("wrld_simpl", package = "maptools")
WGS84 = crs(wrld_simpl) # geographic projection
idd = subset (wrld_simpl, NAME == 'India')
world2 = readOGR ('./shapes/ne_10m_admin_0_countries.shp')
plot (world2[which(world2@data$SUBREGION=='Southern Asia'),])
sa1 = subset (world2, SUBREGION == 'Southern Asia')
plot (sa1)
wc1 = getData('worldclim', var='bio', res=10)
wc2 = wc1[[c(1,4,12)]]
sa2 = crop (wc2, sa1) #cropping extracts the extent of the area studied
plot (sa2)
sa2 = mask (sa2, sa1) #masking delineates the shape file in the raster
plot (sa2)
sa2
sa2
mn = cellStats(sa2[[1]], mean) #obtaining the mean of BIO1 in the Southern Asia region
st_dev = cellStats(sa2[[1]], sd) #obtaining the standard deviation of BIO1 in the Southern Asia region
mn
st_dev
sa2
mn2 = cellStats(sa2[[3]], mean) #obtaining the mean of BIO12 in the Southern Asia region
st_dev2 = cellStats(sa2[[3]], sd) #obtaining the standard deviation of BIO12 in the Southern Asia region
mn2
st_dev2
my.parameters = formatFunctions(bio1 = c(fun = 'dnorm', mean = 300, sd = 50),
bio12 = c(fun = 'dnorm', mean = 600, sd = 200),
rescale = T) #we transformed to TRUE because I am using two variables
my.parameters
species1 = generateSpFromFun(raster.stack = sa2[[c(1,3)]], parameters = my.parameters,
rescale.each.response = T, #we transformed to TRUE because I am using two variables
rescale = T, #we transformed to TRUE because I am using two variables
plot = TRUE)
plotResponse(species1)
cellStats(species1$suitab.raster, stat = "mean") #default prevalence, the mean of the suitability
species1.prev1 <- convertToPA(species1,
PA.method = "probability",
prob.method = "linear") #uses the default prevalence, the mean of the suit rasters!
plot (species1.prev1$pa.raster)
species2.prev2 <- convertToPA(species1,
PA.method = "probability",
prob.method = "linear",
species.prevalence = 0.05) #user define prevalence, has to be lower than predefined prevalence!
species2.prev2$probability.of.occurrence
par (mfrow = c(1,1))
plot (species2.prev2$pa.raster)
plot (species2.prev2$suitab.raster)
plot (species2.prev2$probability.of.occurrence)
occs2 <- sampleOccurrences(species2.prev2, 200, type = "presence only", #can be changed to presence/absence
plot = TRUE, extract.probability = T)
occs2$sample.points
plot(species2.prev2$probability.of.occurrence)
points (occs2$sample.points[,1:2][which (occs2$sample.points[,4]==1),],
col = 'red', cex = 0.3, pch = 3)
sp_points = occs2$sample.points[,1:2][which (occs2$sample.points[,4]==1),] #vector of occurrences
sp_points
v_species1 = raster('v_species_niche1.asc')
tp1 = read.csv ('tp1_points.csv', header= T)
tp1$ind =  paste(tp1[,1], tp1[,2], sep = "_") #new column for reference
#' from the true observations, we collect only the 35% that are available from
#' a true atual sampling scheme, assuming non-biased excercises:
#'
rsp = tp1[sample(nrow(tp1),(0.35*nrow(tp1))),]
dim(rsp)
ind_p =  tp1[!tp1[,3] %in% rsp[,3],] #points not sampled in the true occurrence data
dim(ind_p)
#' we select again the 35% of the points not used in the distribution to obtain an independent set of points
ind_p2 = ind_p[sample(nrow(ind_p),(0.25*nrow(ind_p))),]
ind_p2$sp1 = rep ('sp1', length(ind_p2[,1]))
ind_p2 = ind_p2[,(c(4,1,2,3))]
dim(ind_p2)
tp1$ind = NULL
rsp$ind=NULL
ind_p2$ind = NULL
plot (v_species1)
points (tp1, col = 'red', pch = 3, cex = 0.6) #points from the truth distribution
points (rsp, col = 'blue', pch = 16, cex = 0.6) #points for modeling
points (ind_p2, col = 'black', pch = 16, cex = 0.6) #points for evaluation
dim(model_pts2) #64 points to work
model_pts2 = read.csv ('./thining/model_pts_thin1.csv', header = T)
dim(model_pts2) #64 points to work
model_pts2$check = paste(model_pts2[,2], model_pts2[,3], sep = "_")
train = model_pts2[sample(nrow(model_pts2), round((length(model_pts2[,1])/2))),]
test = model_pts2[!model_pts2[,4] %in% train[,4],]
dim(train)
dim(test)
model_pts2$check = NULL
train$check = NULL
test$check = NULL
plot (sa1)
points (train[,2:3], col = 'red', pch = 3, cex = 0.6) #training points
points (test[,2:3], col = 'blue', pch = 16, cex = 0.6) #evaluation points
model_pts3 = read.csv ('model_pts_def.csv', header = T)
dim(model_pts3)
occ_sp = SpatialPointsDataFrame(coords = model_pts3[,2:3],
data = model_pts3,proj4string = WGS84)
plot (occ_sp)
plot(sa1)
e = drawExtent()
cp = as(extent(e), 'SpatialPolygons')
proj4string(cp) = WGS84 #project extent to the same projection as wrld_simpl map
mask1 = crop (sa1, cp)
plot(mask1)
ch_index = chull (x = model_pts3[,2], y = model_pts3[,3]) #obtaining the indexes of the convex hulls for the overlap
ver_t = model_pts3[,2][c(ch_index)] #vetices on longitude
ver_h = model_pts3[,3][c(ch_index)] #vetices on latitude
ch = convHull(cbind (ver_t, ver_h)) #create the convex hull
plot (ch, add = T, border = 'red')
plot(sa1)
plot (ch, add = T, border = 'red')
model_pts3 = read.csv ('model_pts_def.csv', header = T)
dim(model_pts3)
occ_sp = SpatialPointsDataFrame(coords = model_pts3[,2:3],
data = model_pts3,proj4string = WGS84)
plot(sa1)
e = drawExtent()
plot(sa1)
plot (occ_sp, add = T)
plot(sa1)
plot (occ_sp, add = T, pch = 3, cex = 0.6)
plot (occ_sp, add = T, pch = 3, cex = 0.6, col = 'red')
e = drawExtent()
cp = as(extent(e), 'SpatialPolygons')
proj4string(cp) = WGS84 #project extent to the same projection as wrld_simpl map
mask1 = crop (sa1, cp)
plot(mask1)
plot (occ_sp, add = T, pch = 3, cex = 0.6, col = 'red')
ch_index = chull (x = model_pts3[,2], y = model_pts3[,3]) #obtaining the indexes of the convex hulls for the overlap
ver_t = model_pts3[,2][c(ch_index)] #vetices on longitude
ver_h = model_pts3[,3][c(ch_index)] #vetices on latitude
ch = convHull(cbind (ver_t, ver_h)) #create the convex hull
plot(sa1)
plot (occ_sp, add = T, pch = 3, cex = 0.6, col = 'red')
plot (ch, add = T, border = 'red')
mask2 = crop (sa1, ch@polygons)
plot (mask2)
#visualization
plot(mask1)
plot (occ_sp, add = T, pch = 3, cex = 0.6, col = 'red')
ch_index = chull (x = model_pts3[,2], y = model_pts3[,3]) #obtaining the indexes of the convex hulls for the overlap
ver_t = model_pts3[,2][c(ch_index)] #vetices on longitude
ver_h = model_pts3[,3][c(ch_index)] #vetices on latitude
dd = 500000 #Define the distance buffer in meters because it has a WGS84 projection!
buff1 = buffer(occ_sp, width = dd, dissolve = T)
plot (sa1)
plot (occ_sp, add = T, col = 'blue')
plot (buff1, add = T, border = 'red')
cnt = data.frame(mean(occ_sp$x),mean(occ_sp$y)) #obtaining the centroid of points
dist = mean(pointDistance(occ_sp, cnt, longlat = T)) #obtaining the mean across the distances of all occurrences to the centroid
buff2 = buffer(occ_sp, width = dist, dissolve = T) #create the buffer
plot (buff2, add= T, border = 'red')
points (cnt, pch = 16, cex = 2)
wc1
buff2
buf_ras = crop(wc1, buff2)
buf_ras = mask (buf_ras, buff2)
plot (buf_ras[[1]])
rc = layerStats(buf_ras, 'pearson', na.rm = T)
class(rc) #list with two objects, the correlation values and the mean of each variable
dat = data.frame (rc[2]) #transform the second element in a dataframe
names = row.names (dat) #obtain the names
length(names)
mat = matrix (unlist (rc, recursive = F, use.names = T), ncol = 19, byrow = T)
colnames(mat) = names #adding the names to the columns
rownames(mat) = c(names, 'mean') #addig names to the rows
View (mat) #checking the correlation matrix
mat[mat[,1] <0.8,]
mat_def = mat[-nrow(mat),] #matrix without the mean
dev.new()
corrplot (mat_def, method = 'number', type = 'upper',
tl.col = 'black', is.corr = F)
un_wc1 = buf_ras[[c(1,2,7,12, 14, 15)]]
un_wc1 = buf_ras[[c(1,2,7,12, 14, 15)]]
sel_vars = wc1[[c(1,2,7,12, 14, 15)]]
un_project = crop (sel_vars, sa1) #using the original mask
un_project = mask (un_project, sa1)
pca_vs= stack(list.files('./PCA_vs/Initial', full.names = T, pattern = 'asc'))
pc_res1 = read.table ('./PCA_vs/Initial/pca_results.txt', header = T, sep = '\t')
plot (pca_vs)
ne_mod = raster ('./Final_Models/M_0.1_F_lq_Set_2_NE/sp1_se_asia_median.asc')
setwd('./KUENM')
getwd()
ne_mod = raster ('./Final_Models/M_0.1_F_lq_Set_2_NE/sp1_se_asia_median.asc')
ne_max = raster ('./Final_Models/M_0.1_F_lq_Set_2_NE/sp1_se_asia_max.asc')
ne_min = raster ('./Final_Models/M_0.1_F_lq_Set_2_NE/sp1_se_asia_min.asc')
uncer_mod = ne_max - ne_min
color = colorRampPalette(c('#c90020', 'azure2', '#0571b1'))
par (mfrow = c(1,3))
plot(species2.prev2$probability.of.occurrence, main = 'Pathogen HK')
plot (ne_mod, main = 'KUENM model')
plot (ne_th1, main = 'binarized')
plot (uncer_mod, col= color(100), main = 'uncertainty (range)')
plot(species2.prev2$probability.of.occurrence, main = 'Pathogen HK')
plot (ne_mod, main = 'KUENM model')
par (mfrow = c(1,1))
plot(species2.prev2$probability.of.occurrence, main = 'Pathogen HK')
plot (ne_mod, main = 'KUENM model')
plot (ne_th1, main = 'binarized')
suit_vals = extract (ne_mod, model_pts3[,2:3])
th1 = quantile(suit_vals, 0.05)
ne_th1 = ne_mod >= th1
plot (ne_th1)
points (model_pts3[,2:3], col = 'red', pch = 4, cex = 0.3) #calibration points
points (ind_p2[,2:3],col = 'blue', pch = 3, cex = 0.3) #independent points
plot (ne_mod, main = 'KUENM model')
plot (ne_mod, main = 'KUENM model')
plot (ne_th1, main = 'binarized')
plot (uncer_mod, col= color(100), main = 'uncertainty (range)')
plot (ne_mod, main = 'KUENM model')
points (model_pts3[,2:3], col = 'red', pch = 4, cex = 0.3)
points (ind_p2[,2:3],col = 'blue', pch = 3, cex = 0.3)
points (train[,2:3], col = 'red', pch = 3, cex = 0.6) #training points
plot (sa1)
points (train[,2:3], col = 'red', pch = 3, cex = 0.6) #training points
plot (sa1)
points (test[,2:3], col = 'blue', pch = 16, cex = 0.6) #evaluation points
dim(train)
dim(test)
dim(train)
dim(test)
ind_p2
ne_mod
max_default
enmeval_sel
getwd ()
setwd ('"/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling-/eval_diff_models')
setwd ('"/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling/eval_diff_models')
setwd ("/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling/eval_diff_models")
setwd ('/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling-/eval_diff_models')
ind_p2
setwd ('/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling-/eval_diff_models')
enmeval_sel = raster ('enmeval_cont.asc')
setwd ('/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling-/')
enmeval_sel = raster ('/eval_diff_models/enmeval_cont.asc')
enmeval_sel = raster ('./eval_diff_models/enmeval_cont.asc')
max_default = raster ('./eval_diff_models/sp1_se_asia_median_default.asc')
plot (enmeval_sel)
plot (max_default)
plot (enmeval_sel)
enmeval_bin = raster ('./eval_diff_models/enmeval_bin.asc')
svm_geo = raster ('./eval_diff_models/hypervolum_svm.asc')
default_th = raster ('./eval_diff_models/default_th5.asc')
ne_th1 = raster ('./eval_diff_models/ne_th5.asc')
ne_mod = raster ('./eval_diff_models/sp1_se_asia_median_default.asc')
max_default = raster ('./eval_diff_models/sp1_se_asia_median_default.asc')
plot (enmeval_sel)
plot (max_default)
plot (ne_mod)
plot (max_default)
plot (ne_mod)
plot (enmeval_sel)
max_default = raster ('./eval_diff_models/sp1_se_asia_median_original.asc')
plot (enmeval_sel)
plot (max_default)
plot (ne_mod)
plot (enmeval_sel)
plot (ne_mod)
plot (max_default)
plot (enmeval_sel)
plot (ne_th1)
plot (default_th)
plot (enmeval_bin)
plot (svm_geo)
ind_p2
ne_th1
default_th
enmeval_bin
svm_geo
#' we use the independent points to extract the 1/0 values from the binarized model
#' and then we obtain the average of that information
sens1 = mean(extract(ne_th1, ind_p2[,2:3])) #sensitivity, number of points correctly predicted as present
omr1 = 1-sens1 #omission rate, 1-sensitivity
omr1
or_res1 = kuenm_omrat (model = ne_mod, occ.tra = model_pts3[,2:3],
occ.test = ind_p2[,2:3], threshold = 5)
or_res1
sens2 = mean(extract(default_th, ind_p2[,2:3])) #sensitivity, number of points correctly predicted as present
omr2 = 1-sens2 #omission rate, 1-sensitivity
omr2
sens3 = mean(extract(enmeval_bin, ind_p2[,2:3])) #sensitivity, number of points correctly predicted as present
omr3 = 1-sens3 #omission rate, 1-sensitivity
omr3
sens4 = mean(extract(svm_geo, ind_p2[,2:3])) #sensitivity, number of points correctly predicted as present
omr4 = 1-sens4 #omission rate, 1-sensitivity
omr4
mean(extract(svm_geo, ind_p2[,2:3]))
omr4
setwd ('/Users/daniel/Documents/LEPTOSPIROSIS')
lep1 = read.table ('egresosh-2000-2019.csv', header = T, sep =';',fill = T, encoding = 'LATIN1')
View(lep1)
setwd ('/Users/daniel/Documents/CURSO_INDIA/Divakar1/z')
setwd ('/Users/daniel/Documents/CURSO_INDIA/Divakar1/')
setwd ('/Users/daniel/Documents/CURSO_INDIA/Divakar1/z')
modisd = stack (list.files(full.names = T))
modisd
modisd
modisd
base_layer = raster (file.choose())
base_layer
results1 = resample (modisd, base_layer, method = 'bilinear')
results1
base_layer
plot (results1[[1]])
plot (base_layer)
plot (results1[[1]], base_layer)
plot (results1[[1]])
plot (base_layer)
dev.new()
plot (results1[[1]])
dev.new()
plot (base_layer)
setwd ('/Users/daniel/Documents/CURSO_INDIA/Divakar1/')
getwd()
writeRaster(results1, filename = 'modis1', format = 'ascii',
bylayer = T, suffix = 'modis1', overwrite = T,
NAflag = -9999)
writeRaster(results1, filename = 'modis1', format = 'ascii',
bylayer = T, suffix = 'modis2', overwrite = T,
NAflag = -9999)
writeRaster(results1, filename = 'modis1', format = 'ascii',
bylayer = T, overwrite = T,
NAflag = -9999)
getwd()
setwd ('/Users/daniel/Documents/CURSO_INDIA/Divakar1/z')
setwd ('/Users/daniel/Documents/CURSO_INDIA/Divakar1/')
getwd()
modisd = stack (list.files(path = './z',full.names = T))
modisd
base_layer = raster (list.files(path = './base_layer',full.names = T))
results1
writeRaster(results1, filename = 'modis1', format = 'ascii',
bylayer = T, suffix = names (results1), overwrite = T,
NAflag = -9999)
setwd ('/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling-')
WGS84
plot (wrld_simpl)
plot (buff2, add= T, border = 'red')
plot (wrld_simpl)
world2 = readOGR ('./shapes/ne_10m_admin_0_countries.shp')
sa1 = subset (world2, SUBREGION == 'Southern Asia')
plot (sa1)
cnt = data.frame(mean(occ_sp$x),mean(occ_sp$y)) #obtaining the centroid of points
cnt
dist = mean(pointDistance(occ_sp, cnt, longlat = T)) #obtaining the mean across the distances of all occurrences to the centroid
buff2 = buffer(occ_sp, width = dist, dissolve = T) #create the buffer
plot (buff2, add= T, border = 'red')
points (cnt, pch = 16, cex = 2)
writeOGR(buff2, layer= 'buff2',
dsn = './BUFF2', driver = 'ESRI Shapefile')
buff2
buff2 = buffer(occ_sp, width = dist, dissolve = T) #create the buffer
df = data.frame (matrix(nrow = 1, ncol = 1, 1)) #fake dataframe to convert the spatial polygon to a spatial polygon dataframe...only this way can be written...
buff2 = SpatialPolygonsDataFrame(buff2, df, match.ID = F) #transforming in correct object
buff2
plot (buff2, add= T, border = 'red')
points (cnt, pch = 16, cex = 2)
writeOGR(buff2, layer= 'buff2',
dsn = './BUFF2', driver = 'ESRI Shapefile')
setwd ('/Users/daniel/Documents/CURSO_INDIA/Divakar1/')
getwd()
modisd = stack (list.files(path = './z',full.names = T))
cliping_lay1 = readOGR (file.choose()) #select the .shp file, it has to have the rest of the corresponding shape files around (i.e., .dbf, .prj)
cliping_lay1
plot (cliping_lay1)
crop_files = crop (modisd, cliping_lay1) #cropping = match the extend of the shape file
crop_files
plot (crop_files[[1]])
mask_files = mask (crop_files, cliping_lay1) #masking = match the contour of the shape file
plot (mask_files[[1]])
writeRaster(mask_files, filename = 'mask_files', format = 'ascii',
bylayer = T, suffix = names (mask_files), overwrite = T,
NAflag = -9999)
names (mask_files)
writeRaster(mask_files, filename = 'masked1', format = 'ascii',
bylayer = T, suffix = names (mask_files), overwrite = T,
NAflag = -9999)
mask_files[[1]]
setwd ('/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling-')
setwd('./KUENM')
model_pts3 = read.csv ('model_pts_def.csv', header = T)
train = read.csv ('in_train.csv', header = T)
test = read.csv ('in_test.csv', header= T)
occ_joint <- "model_pts_def.csv"
occ_joint
occ_tra <- "in_train.csv"
occ_tra
M_var_dir <- "M_variables"
M_var_dir
batch_cal <- "Candidate_models"
out_dir <- "Candidate_Models"
f_clas <- c("l", 'lq', 'lqp')
maxent_path <- '/Users/daniel/Documents/GitHub/Gadgets-and-scripts-for-niche-modeling-/KUENM'
wait <- FALSE
run <- TRUE
